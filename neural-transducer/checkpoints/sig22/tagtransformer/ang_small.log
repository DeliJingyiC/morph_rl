INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: seed - 0
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: train - ['/users/PAS2062/delijingyic/project/morph/neural-transducer/2022InflectionST/part1/development_languages/ang_small.train']
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: dev - ['/users/PAS2062/delijingyic/project/morph/neural-transducer/2022InflectionST/part1/development_languages/ang.dev']
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: test - ['../../../part1/submissions/baseline/wuetal-transformer/ang.test']
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: model - 'checkpoints/sig22/tagtransformer/ang_small'
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: load - ''
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: bs - 400
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: epochs - 20
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: max_steps - 20000
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: warmup_steps - 4000
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: total_eval - 50
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: optimizer - <Optimizer.adam: 'adam'>
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: scheduler - <Scheduler.warmupinvsqr: 'warmupinvsqr'>
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: lr - 0.001
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: min_lr - 1e-05
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: momentum - 0.9
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: beta1 - 0.9
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: beta2 - 0.98
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: estop - 1e-08
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: cooldown - 0
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: patience - 0
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: discount_factor - 0.5
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: max_norm - 1.0
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: gpuid - [0]
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: loglevel - 'info'
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: saveall - False
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: shuffle - True
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: cleanup_anyway - True
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: dataset - <Data.sigmorphon17task1: 'sigmorphon17task1'>
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: max_seq_len - 128
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: max_decode_len - 32
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: decode_beam_size - 5
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: init - ''
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: dropout - 0.3
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: embed_dim - 256
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: nb_heads - 4
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: src_layer - 4
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: trg_layer - 4
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: src_hs - 1024
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: trg_hs - 1024
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: label_smooth - 0.1
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: tie_trg_embed - False
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: arch - <Arch.tagtransformer: 'tagtransformer'>
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: nb_sample - 2
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: wid_siz - 11
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: indtag - False
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: decode - <Decode.greedy: 'greedy'>
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: mono - False
INFO - 08/29/23 13:40:35 - 0:00:00 - command line argument: bestacc - True
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: seed - 0
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: train - ['/users/PAS2062/delijingyic/project/morph/neural-transducer/2022InflectionST/part1/development_languages/ang_small.train']
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: dev - ['/users/PAS2062/delijingyic/project/morph/neural-transducer/2022InflectionST/part1/development_languages/ang.dev']
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: test - ['/users/PAS2062/delijingyic/project/morph/neural-transducer/2022InflectionST/part1/development_languages/ang.test']
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: model - 'checkpoints/sig22/tagtransformer/ang_small'
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: load - ''
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: bs - 400
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: epochs - 20
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: max_steps - 20000
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: warmup_steps - 4000
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: total_eval - 50
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: optimizer - <Optimizer.adam: 'adam'>
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: scheduler - <Scheduler.warmupinvsqr: 'warmupinvsqr'>
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: lr - 0.001
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: min_lr - 1e-05
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: momentum - 0.9
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: beta1 - 0.9
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: beta2 - 0.98
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: estop - 1e-08
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: cooldown - 0
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: patience - 0
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: discount_factor - 0.5
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: max_norm - 1.0
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: gpuid - [0]
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: loglevel - 'info'
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: saveall - False
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: shuffle - True
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: cleanup_anyway - True
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: dataset - <Data.sigmorphon17task1: 'sigmorphon17task1'>
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: max_seq_len - 128
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: max_decode_len - 32
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: decode_beam_size - 5
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: init - ''
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: dropout - 0.3
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: embed_dim - 256
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: nb_heads - 4
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: src_layer - 4
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: trg_layer - 4
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: src_hs - 1024
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: trg_hs - 1024
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: label_smooth - 0.1
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: tie_trg_embed - False
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: arch - <Arch.tagtransformer: 'tagtransformer'>
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: nb_sample - 2
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: wid_siz - 11
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: indtag - False
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: decode - <Decode.greedy: 'greedy'>
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: mono - False
INFO - 08/29/23 13:44:02 - 0:00:00 - command line argument: bestacc - True
INFO - 08/29/23 13:44:02 - 0:00:00 - src vocab size 68
INFO - 08/29/23 13:44:02 - 0:00:00 - trg vocab size 43
INFO - 08/29/23 13:44:02 - 0:00:00 - src vocab ['<PAD>', '<BOS>', '<EOS>', '<UNK>', 'B', 'E', 'F', 'I', 'R', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'w', 'y', 'æ', 'ð', 'þ', 'ā', 'ċ', 'ē', 'ġ', 'ī', 'ō', 'œ', 'ū', 'ǣ', 'ȳ', '̄', '1', '2', '3', 'ACC', 'ADJ', 'DAT', 'FEM', 'GEN', 'IMP', 'IND', 'INS', 'LGSPEC01', 'LGSPEC02', 'MASC', 'MASC+FEM', 'N', 'NEUT', 'NFIN', 'NOM', 'PL', 'PRS', 'PST', 'SBJV', 'SG', 'V']
INFO - 08/29/23 13:44:02 - 0:00:00 - trg vocab ['<PAD>', '<BOS>', '<EOS>', '<UNK>', 'B', 'E', 'F', 'I', 'R', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'w', 'y', 'æ', 'ð', 'þ', 'ā', 'ċ', 'ē', 'ġ', 'ī', 'ō', 'œ', 'ū', 'ǣ', 'ȳ', '̄']
INFO - 08/29/23 13:44:04 - 0:00:02 - model: TagTransformer(
                                       (src_embed): Embedding(68, 256, padding_idx=0)
                                       (trg_embed): Embedding(43, 256, padding_idx=0)
                                       (position_embed): SinusoidalPositionalEmbedding()
                                       (encoder): TransformerEncoder(
                                         (layers): ModuleList(
                                           (0-3): 4 x TransformerEncoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                         )
                                         (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                       )
                                       (decoder): TransformerDecoder(
                                         (layers): ModuleList(
                                           (0-3): 4 x TransformerDecoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (multihead_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                         )
                                         (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                       )
                                       (final_out): Linear(in_features=256, out_features=43, bias=True)
                                       (dropout): Dropout(p=0.3, inplace=False)
                                       (special_embeddings): Embedding(2, 256)
                                     )
INFO - 08/29/23 13:44:04 - 0:00:02 - number of parameter 7413803
INFO - 08/29/23 13:44:04 - 0:00:02 - maximum training 20000 steps (10000 epochs)
INFO - 08/29/23 13:44:04 - 0:00:02 - evaluate every 200 epochs
INFO - 08/29/23 13:44:04 - 0:00:02 - At 0-th epoch with lr 0.000000.
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: seed - 0
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: train - ['/users/PAS2062/delijingyic/project/morph/neural-transducer/2022InflectionST/part1/development_languages/ang_small.train']
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: dev - ['/users/PAS2062/delijingyic/project/morph/neural-transducer/2022InflectionST/part1/development_languages/ang.dev']
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: test - ['/users/PAS2062/delijingyic/project/morph/neural-transducer/2022InflectionST/part1/development_languages/ang.test']
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: model - 'checkpoints/sig22/tagtransformer/ang_small'
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: load - ''
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: bs - 400
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: epochs - 20
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: max_steps - 20000
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: warmup_steps - 4000
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: total_eval - 50
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: optimizer - <Optimizer.adam: 'adam'>
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: scheduler - <Scheduler.warmupinvsqr: 'warmupinvsqr'>
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: lr - 0.001
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: min_lr - 1e-05
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: momentum - 0.9
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: beta1 - 0.9
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: beta2 - 0.98
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: estop - 1e-08
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: cooldown - 0
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: patience - 0
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: discount_factor - 0.5
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: max_norm - 1.0
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: gpuid - [0]
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: loglevel - 'info'
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: saveall - False
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: shuffle - True
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: cleanup_anyway - True
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: dataset - <Data.sigmorphon17task1: 'sigmorphon17task1'>
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: max_seq_len - 128
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: max_decode_len - 32
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: decode_beam_size - 5
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: init - ''
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: dropout - 0.3
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: embed_dim - 256
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: nb_heads - 4
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: src_layer - 4
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: trg_layer - 4
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: src_hs - 1024
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: trg_hs - 1024
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: label_smooth - 0.1
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: tie_trg_embed - False
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: arch - <Arch.tagtransformer: 'tagtransformer'>
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: nb_sample - 2
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: wid_siz - 11
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: indtag - False
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: decode - <Decode.greedy: 'greedy'>
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: mono - False
INFO - 08/31/23 18:25:18 - 0:00:00 - command line argument: bestacc - True
INFO - 08/31/23 18:25:18 - 0:00:00 - src vocab size 68
INFO - 08/31/23 18:25:18 - 0:00:00 - trg vocab size 43
INFO - 08/31/23 18:25:18 - 0:00:00 - src vocab ['<PAD>', '<BOS>', '<EOS>', '<UNK>', 'B', 'E', 'F', 'I', 'R', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'w', 'y', 'æ', 'ð', 'þ', 'ā', 'ċ', 'ē', 'ġ', 'ī', 'ō', 'œ', 'ū', 'ǣ', 'ȳ', '̄', '1', '2', '3', 'ACC', 'ADJ', 'DAT', 'FEM', 'GEN', 'IMP', 'IND', 'INS', 'LGSPEC01', 'LGSPEC02', 'MASC', 'MASC+FEM', 'N', 'NEUT', 'NFIN', 'NOM', 'PL', 'PRS', 'PST', 'SBJV', 'SG', 'V']
INFO - 08/31/23 18:25:18 - 0:00:00 - trg vocab ['<PAD>', '<BOS>', '<EOS>', '<UNK>', 'B', 'E', 'F', 'I', 'R', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'w', 'y', 'æ', 'ð', 'þ', 'ā', 'ċ', 'ē', 'ġ', 'ī', 'ō', 'œ', 'ū', 'ǣ', 'ȳ', '̄']
INFO - 08/31/23 18:25:21 - 0:00:03 - model: TagTransformer(
                                       (src_embed): Embedding(68, 256, padding_idx=0)
                                       (trg_embed): Embedding(43, 256, padding_idx=0)
                                       (position_embed): SinusoidalPositionalEmbedding()
                                       (encoder): TransformerEncoder(
                                         (layers): ModuleList(
                                           (0-3): 4 x TransformerEncoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                         )
                                         (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                       )
                                       (decoder): TransformerDecoder(
                                         (layers): ModuleList(
                                           (0-3): 4 x TransformerDecoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (multihead_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                         )
                                         (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                       )
                                       (final_out): Linear(in_features=256, out_features=43, bias=True)
                                       (dropout): Dropout(p=0.3, inplace=False)
                                       (special_embeddings): Embedding(2, 256)
                                     )
INFO - 08/31/23 18:25:21 - 0:00:03 - number of parameter 7413803
INFO - 08/31/23 18:25:21 - 0:00:03 - maximum training 20000 steps (10000 epochs)
INFO - 08/31/23 18:25:21 - 0:00:03 - evaluate every 200 epochs
INFO - 08/31/23 18:25:21 - 0:00:03 - At 0-th epoch with lr 0.000000.
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: seed - 0
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: train - ['/users/PAS2062/delijingyic/project/morph/neural-transducer/2022InflectionST/part1/development_languages/ang_small.train']
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: dev - ['/users/PAS2062/delijingyic/project/morph/neural-transducer/2022InflectionST/part1/development_languages/ang.dev']
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: test - ['/users/PAS2062/delijingyic/project/morph/neural-transducer/2022InflectionST/part1/development_languages/ang.test']
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: model - 'checkpoints/sig22/tagtransformer/ang_small'
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: load - ''
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: bs - 400
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: epochs - 20
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: max_steps - 20000
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: warmup_steps - 4000
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: total_eval - 50
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: optimizer - <Optimizer.adam: 'adam'>
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: scheduler - <Scheduler.warmupinvsqr: 'warmupinvsqr'>
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: lr - 0.001
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: min_lr - 1e-05
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: momentum - 0.9
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: beta1 - 0.9
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: beta2 - 0.98
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: estop - 1e-08
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: cooldown - 0
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: patience - 0
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: discount_factor - 0.5
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: max_norm - 1.0
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: gpuid - [0]
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: loglevel - 'info'
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: saveall - False
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: shuffle - True
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: cleanup_anyway - True
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: dataset - <Data.sigmorphon17task1: 'sigmorphon17task1'>
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: max_seq_len - 128
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: max_decode_len - 32
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: decode_beam_size - 5
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: init - ''
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: dropout - 0.3
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: embed_dim - 256
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: nb_heads - 4
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: src_layer - 4
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: trg_layer - 4
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: src_hs - 1024
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: trg_hs - 1024
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: label_smooth - 0.1
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: tie_trg_embed - False
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: arch - <Arch.tagtransformer: 'tagtransformer'>
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: nb_sample - 2
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: wid_siz - 11
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: indtag - False
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: decode - <Decode.greedy: 'greedy'>
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: mono - False
INFO - 08/31/23 18:31:59 - 0:00:00 - command line argument: bestacc - True
INFO - 08/31/23 18:31:59 - 0:00:00 - src vocab size 68
INFO - 08/31/23 18:31:59 - 0:00:00 - trg vocab size 43
INFO - 08/31/23 18:31:59 - 0:00:00 - src vocab ['<PAD>', '<BOS>', '<EOS>', '<UNK>', 'B', 'E', 'F', 'I', 'R', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'w', 'y', 'æ', 'ð', 'þ', 'ā', 'ċ', 'ē', 'ġ', 'ī', 'ō', 'œ', 'ū', 'ǣ', 'ȳ', '̄', '1', '2', '3', 'ACC', 'ADJ', 'DAT', 'FEM', 'GEN', 'IMP', 'IND', 'INS', 'LGSPEC01', 'LGSPEC02', 'MASC', 'MASC+FEM', 'N', 'NEUT', 'NFIN', 'NOM', 'PL', 'PRS', 'PST', 'SBJV', 'SG', 'V']
INFO - 08/31/23 18:31:59 - 0:00:00 - trg vocab ['<PAD>', '<BOS>', '<EOS>', '<UNK>', 'B', 'E', 'F', 'I', 'R', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'w', 'y', 'æ', 'ð', 'þ', 'ā', 'ċ', 'ē', 'ġ', 'ī', 'ō', 'œ', 'ū', 'ǣ', 'ȳ', '̄']
INFO - 08/31/23 18:32:02 - 0:00:03 - model: TagTransformer(
                                       (src_embed): Embedding(68, 256, padding_idx=0)
                                       (trg_embed): Embedding(43, 256, padding_idx=0)
                                       (position_embed): SinusoidalPositionalEmbedding()
                                       (encoder): TransformerEncoder(
                                         (layers): ModuleList(
                                           (0-3): 4 x TransformerEncoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                         )
                                         (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                       )
                                       (decoder): TransformerDecoder(
                                         (layers): ModuleList(
                                           (0-3): 4 x TransformerDecoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (multihead_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                         )
                                         (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                       )
                                       (final_out): Linear(in_features=256, out_features=43, bias=True)
                                       (dropout): Dropout(p=0.3, inplace=False)
                                       (special_embeddings): Embedding(2, 256)
                                     )
INFO - 08/31/23 18:32:02 - 0:00:03 - number of parameter 7413803
INFO - 08/31/23 18:32:02 - 0:00:03 - maximum training 20000 steps (10000 epochs)
INFO - 08/31/23 18:32:02 - 0:00:03 - evaluate every 200 epochs
INFO - 08/31/23 18:32:02 - 0:00:03 - At 0-th epoch with lr 0.000000.
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: seed - 0
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: train - ['/users/PAS2062/delijingyic/project/morph/neural-transducer/2022InflectionST/part1/development_languages/ang_small.train']
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: dev - ['/users/PAS2062/delijingyic/project/morph/neural-transducer/2022InflectionST/part1/development_languages/ang.dev']
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: test - ['/users/PAS2062/delijingyic/project/morph/neural-transducer/2022InflectionST/part1/development_languages/ang.test']
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: model - 'checkpoints/sig22/tagtransformer/ang_small'
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: load - ''
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: bs - 400
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: epochs - 20
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: max_steps - 20000
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: warmup_steps - 4000
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: total_eval - 50
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: optimizer - <Optimizer.adam: 'adam'>
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: scheduler - <Scheduler.warmupinvsqr: 'warmupinvsqr'>
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: lr - 0.001
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: min_lr - 1e-05
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: momentum - 0.9
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: beta1 - 0.9
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: beta2 - 0.98
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: estop - 1e-08
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: cooldown - 0
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: patience - 0
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: discount_factor - 0.5
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: max_norm - 1.0
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: gpuid - [0]
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: loglevel - 'info'
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: saveall - False
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: shuffle - True
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: cleanup_anyway - True
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: dataset - <Data.sigmorphon17task1: 'sigmorphon17task1'>
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: max_seq_len - 128
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: max_decode_len - 32
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: decode_beam_size - 5
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: init - ''
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: dropout - 0.3
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: embed_dim - 256
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: nb_heads - 4
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: src_layer - 4
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: trg_layer - 4
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: src_hs - 1024
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: trg_hs - 1024
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: label_smooth - 0.1
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: tie_trg_embed - False
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: arch - <Arch.tagtransformer: 'tagtransformer'>
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: nb_sample - 2
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: wid_siz - 11
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: indtag - False
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: decode - <Decode.greedy: 'greedy'>
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: mono - False
INFO - 08/31/23 18:37:23 - 0:00:00 - command line argument: bestacc - True
INFO - 08/31/23 18:37:23 - 0:00:00 - src vocab size 68
INFO - 08/31/23 18:37:23 - 0:00:00 - trg vocab size 43
INFO - 08/31/23 18:37:23 - 0:00:00 - src vocab ['<PAD>', '<BOS>', '<EOS>', '<UNK>', 'B', 'E', 'F', 'I', 'R', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'w', 'y', 'æ', 'ð', 'þ', 'ā', 'ċ', 'ē', 'ġ', 'ī', 'ō', 'œ', 'ū', 'ǣ', 'ȳ', '̄', '1', '2', '3', 'ACC', 'ADJ', 'DAT', 'FEM', 'GEN', 'IMP', 'IND', 'INS', 'LGSPEC01', 'LGSPEC02', 'MASC', 'MASC+FEM', 'N', 'NEUT', 'NFIN', 'NOM', 'PL', 'PRS', 'PST', 'SBJV', 'SG', 'V']
INFO - 08/31/23 18:37:23 - 0:00:00 - trg vocab ['<PAD>', '<BOS>', '<EOS>', '<UNK>', 'B', 'E', 'F', 'I', 'R', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'w', 'y', 'æ', 'ð', 'þ', 'ā', 'ċ', 'ē', 'ġ', 'ī', 'ō', 'œ', 'ū', 'ǣ', 'ȳ', '̄']
INFO - 08/31/23 18:37:25 - 0:00:02 - model: TagTransformer(
                                       (src_embed): Embedding(68, 256, padding_idx=0)
                                       (trg_embed): Embedding(43, 256, padding_idx=0)
                                       (position_embed): SinusoidalPositionalEmbedding()
                                       (encoder): TransformerEncoder(
                                         (layers): ModuleList(
                                           (0): TransformerEncoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                           (1): TransformerEncoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                           (2): TransformerEncoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                           (3): TransformerEncoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                         )
                                         (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                       )
                                       (decoder): TransformerDecoder(
                                         (layers): ModuleList(
                                           (0): TransformerDecoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
                                             )
                                             (multihead_attn): MultiheadAttention(
                                               (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                           (1): TransformerDecoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
                                             )
                                             (multihead_attn): MultiheadAttention(
                                               (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                           (2): TransformerDecoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
                                             )
                                             (multihead_attn): MultiheadAttention(
                                               (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                           (3): TransformerDecoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
                                             )
                                             (multihead_attn): MultiheadAttention(
                                               (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                         )
                                         (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                       )
                                       (final_out): Linear(in_features=256, out_features=43, bias=True)
                                       (dropout): Dropout(p=0.3, inplace=False)
                                       (special_embeddings): Embedding(2, 256)
                                     )
INFO - 08/31/23 18:37:25 - 0:00:02 - number of parameter 7413803
INFO - 08/31/23 18:37:25 - 0:00:02 - maximum training 20000 steps (10000 epochs)
INFO - 08/31/23 18:37:25 - 0:00:02 - evaluate every 200 epochs
INFO - 08/31/23 18:37:25 - 0:00:02 - At 0-th epoch with lr 0.000000.
INFO - 08/31/23 18:40:24 - 0:03:02 - Running average train loss is 4.58576774597168 at epoch 0
INFO - 08/31/23 18:40:24 - 0:03:02 - At 1-th epoch with lr 0.000000.
INFO - 08/31/23 18:42:40 - 0:05:17 - Running average train loss is 4.591229200363159 at epoch 1
INFO - 08/31/23 18:42:40 - 0:05:17 - At 2-th epoch with lr 0.000001.
INFO - 08/31/23 18:44:53 - 0:07:30 - Running average train loss is 4.5671868324279785 at epoch 2
INFO - 08/31/23 18:44:53 - 0:07:30 - At 3-th epoch with lr 0.000002.
INFO - 08/31/23 18:47:07 - 0:09:44 - Running average train loss is 4.592703342437744 at epoch 3
INFO - 08/31/23 18:47:07 - 0:09:44 - At 4-th epoch with lr 0.000002.
INFO - 08/31/23 18:50:02 - 0:12:39 - Running average train loss is 4.547915935516357 at epoch 4
INFO - 08/31/23 18:50:02 - 0:12:39 - At 5-th epoch with lr 0.000003.
